{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from simple_gan import Generator,Discriminator\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GAN on MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial GANs are sensitive to Hyperparams\n",
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4  # best lr for Adam given by andrew karpathy\n",
    "z_dimension = 64 #128,256\n",
    "image_dim = 28*28*1\n",
    "batch_size = 32\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize GAN and setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Generator and Discriminator\n",
    "discriminator = Discriminator(image_dimension=image_dim).to(device=device)\n",
    "generator = Generator(z_dimension=z_dimension,image_dimension=image_dim).to(device=device)\n",
    "fixed_noise = torch.randn((batch_size,z_dimension)).to(device=device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))  # Actual mean and std for MNIST Dataset: transforms.Normalize((0.1307,),(0.3081,))\n",
    "]) # Normalize image values to [-1,1] \n",
    "dataset = datasets.MNIST(root='./data/',transform=transform,download=True)\n",
    "loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)\n",
    "optim_disc = optim.Adam(discriminator.parameters(),lr=lr)\n",
    "optim_gen = optim.Adam(generator.parameters(),lr=lr)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_fake = SummaryWriter(log_dir=f'./runs/GAN_MNIST/fake')  # fake images\n",
    "writer_real = SummaryWriter(log_dir=f'./runs/GAN_MNIST/real')  # real images\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500] Batch 0/1875                       Loss D: 0.6765, loss G: 0.6590\n",
      "Epoch [1/500] Batch 0/1875                       Loss D: 0.3945, loss G: 1.2289\n",
      "Epoch [2/500] Batch 0/1875                       Loss D: 0.2485, loss G: 1.9642\n",
      "Epoch [3/500] Batch 0/1875                       Loss D: 0.2269, loss G: 1.9172\n",
      "Epoch [4/500] Batch 0/1875                       Loss D: 0.5206, loss G: 1.1317\n",
      "Epoch [5/500] Batch 0/1875                       Loss D: 0.2643, loss G: 1.7039\n",
      "Epoch [6/500] Batch 0/1875                       Loss D: 0.4710, loss G: 1.1259\n",
      "Epoch [7/500] Batch 0/1875                       Loss D: 0.5203, loss G: 1.1240\n",
      "Epoch [8/500] Batch 0/1875                       Loss D: 0.4776, loss G: 1.3763\n",
      "Epoch [9/500] Batch 0/1875                       Loss D: 0.3830, loss G: 1.3702\n",
      "Epoch [10/500] Batch 0/1875                       Loss D: 0.5507, loss G: 1.6203\n",
      "Epoch [11/500] Batch 0/1875                       Loss D: 0.6904, loss G: 1.0627\n",
      "Epoch [12/500] Batch 0/1875                       Loss D: 0.5900, loss G: 1.0919\n",
      "Epoch [13/500] Batch 0/1875                       Loss D: 0.5925, loss G: 1.0328\n",
      "Epoch [14/500] Batch 0/1875                       Loss D: 0.6935, loss G: 0.9797\n",
      "Epoch [15/500] Batch 0/1875                       Loss D: 0.6576, loss G: 0.8015\n",
      "Epoch [16/500] Batch 0/1875                       Loss D: 0.6010, loss G: 1.0065\n",
      "Epoch [17/500] Batch 0/1875                       Loss D: 0.8001, loss G: 1.0069\n",
      "Epoch [18/500] Batch 0/1875                       Loss D: 0.7299, loss G: 1.1525\n",
      "Epoch [19/500] Batch 0/1875                       Loss D: 0.7165, loss G: 0.9687\n",
      "Epoch [20/500] Batch 0/1875                       Loss D: 0.4878, loss G: 1.2752\n",
      "Epoch [21/500] Batch 0/1875                       Loss D: 0.7302, loss G: 0.7555\n",
      "Epoch [22/500] Batch 0/1875                       Loss D: 0.6042, loss G: 1.4466\n",
      "Epoch [23/500] Batch 0/1875                       Loss D: 0.6964, loss G: 0.8953\n",
      "Epoch [24/500] Batch 0/1875                       Loss D: 0.7621, loss G: 0.9681\n",
      "Epoch [25/500] Batch 0/1875                       Loss D: 0.5463, loss G: 1.1061\n",
      "Epoch [26/500] Batch 0/1875                       Loss D: 0.5057, loss G: 1.1927\n",
      "Epoch [27/500] Batch 0/1875                       Loss D: 0.7623, loss G: 0.8774\n",
      "Epoch [28/500] Batch 0/1875                       Loss D: 0.6396, loss G: 1.0462\n",
      "Epoch [29/500] Batch 0/1875                       Loss D: 0.5828, loss G: 1.3601\n",
      "Epoch [30/500] Batch 0/1875                       Loss D: 0.7780, loss G: 1.0470\n",
      "Epoch [31/500] Batch 0/1875                       Loss D: 0.5651, loss G: 1.1188\n",
      "Epoch [32/500] Batch 0/1875                       Loss D: 0.4414, loss G: 1.3971\n",
      "Epoch [33/500] Batch 0/1875                       Loss D: 0.5615, loss G: 1.0288\n",
      "Epoch [34/500] Batch 0/1875                       Loss D: 0.6301, loss G: 0.9354\n",
      "Epoch [35/500] Batch 0/1875                       Loss D: 0.6493, loss G: 1.0802\n",
      "Epoch [36/500] Batch 0/1875                       Loss D: 0.5304, loss G: 1.1668\n",
      "Epoch [37/500] Batch 0/1875                       Loss D: 0.7274, loss G: 0.7307\n",
      "Epoch [38/500] Batch 0/1875                       Loss D: 0.6796, loss G: 1.0664\n",
      "Epoch [39/500] Batch 0/1875                       Loss D: 0.6737, loss G: 1.0730\n",
      "Epoch [40/500] Batch 0/1875                       Loss D: 0.5206, loss G: 1.1039\n",
      "Epoch [41/500] Batch 0/1875                       Loss D: 0.5984, loss G: 1.1032\n",
      "Epoch [42/500] Batch 0/1875                       Loss D: 0.5770, loss G: 1.2214\n",
      "Epoch [43/500] Batch 0/1875                       Loss D: 0.5682, loss G: 1.0675\n",
      "Epoch [44/500] Batch 0/1875                       Loss D: 0.6050, loss G: 1.0506\n",
      "Epoch [45/500] Batch 0/1875                       Loss D: 0.5866, loss G: 1.0295\n",
      "Epoch [46/500] Batch 0/1875                       Loss D: 0.5050, loss G: 1.4603\n",
      "Epoch [47/500] Batch 0/1875                       Loss D: 0.6320, loss G: 0.9746\n",
      "Epoch [48/500] Batch 0/1875                       Loss D: 0.7484, loss G: 1.0416\n",
      "Epoch [49/500] Batch 0/1875                       Loss D: 0.6179, loss G: 0.9964\n",
      "Epoch [50/500] Batch 0/1875                       Loss D: 0.7917, loss G: 0.6639\n",
      "Epoch [51/500] Batch 0/1875                       Loss D: 0.5472, loss G: 0.9763\n",
      "Epoch [52/500] Batch 0/1875                       Loss D: 0.5839, loss G: 0.8995\n",
      "Epoch [53/500] Batch 0/1875                       Loss D: 0.5946, loss G: 1.1553\n",
      "Epoch [54/500] Batch 0/1875                       Loss D: 0.7574, loss G: 0.9100\n",
      "Epoch [55/500] Batch 0/1875                       Loss D: 0.5953, loss G: 0.9725\n",
      "Epoch [56/500] Batch 0/1875                       Loss D: 0.6934, loss G: 0.7969\n",
      "Epoch [57/500] Batch 0/1875                       Loss D: 0.6959, loss G: 0.7452\n",
      "Epoch [58/500] Batch 0/1875                       Loss D: 0.7030, loss G: 0.7817\n",
      "Epoch [59/500] Batch 0/1875                       Loss D: 0.6960, loss G: 1.0234\n",
      "Epoch [60/500] Batch 0/1875                       Loss D: 0.5827, loss G: 1.0003\n",
      "Epoch [61/500] Batch 0/1875                       Loss D: 0.5747, loss G: 0.9563\n",
      "Epoch [62/500] Batch 0/1875                       Loss D: 0.6991, loss G: 0.7711\n",
      "Epoch [63/500] Batch 0/1875                       Loss D: 0.6446, loss G: 0.8606\n",
      "Epoch [64/500] Batch 0/1875                       Loss D: 0.6500, loss G: 0.8900\n",
      "Epoch [65/500] Batch 0/1875                       Loss D: 0.7492, loss G: 0.8049\n",
      "Epoch [66/500] Batch 0/1875                       Loss D: 0.6571, loss G: 0.9043\n",
      "Epoch [67/500] Batch 0/1875                       Loss D: 0.5753, loss G: 0.9776\n",
      "Epoch [68/500] Batch 0/1875                       Loss D: 0.6845, loss G: 0.8949\n",
      "Epoch [69/500] Batch 0/1875                       Loss D: 0.6508, loss G: 0.9593\n",
      "Epoch [70/500] Batch 0/1875                       Loss D: 0.6603, loss G: 0.9133\n",
      "Epoch [71/500] Batch 0/1875                       Loss D: 0.6479, loss G: 0.9785\n",
      "Epoch [72/500] Batch 0/1875                       Loss D: 0.6677, loss G: 0.7648\n",
      "Epoch [73/500] Batch 0/1875                       Loss D: 0.7377, loss G: 0.7562\n",
      "Epoch [74/500] Batch 0/1875                       Loss D: 0.5591, loss G: 1.1126\n",
      "Epoch [75/500] Batch 0/1875                       Loss D: 0.6099, loss G: 0.9464\n",
      "Epoch [76/500] Batch 0/1875                       Loss D: 0.6045, loss G: 0.8560\n",
      "Epoch [77/500] Batch 0/1875                       Loss D: 0.5452, loss G: 1.0703\n",
      "Epoch [78/500] Batch 0/1875                       Loss D: 0.5948, loss G: 0.9774\n",
      "Epoch [79/500] Batch 0/1875                       Loss D: 0.6500, loss G: 0.8614\n",
      "Epoch [80/500] Batch 0/1875                       Loss D: 0.5596, loss G: 1.1791\n",
      "Epoch [81/500] Batch 0/1875                       Loss D: 0.6136, loss G: 1.0337\n",
      "Epoch [82/500] Batch 0/1875                       Loss D: 0.5911, loss G: 1.0466\n",
      "Epoch [83/500] Batch 0/1875                       Loss D: 0.6982, loss G: 1.0248\n",
      "Epoch [84/500] Batch 0/1875                       Loss D: 0.6441, loss G: 0.8587\n",
      "Epoch [85/500] Batch 0/1875                       Loss D: 0.6490, loss G: 0.8171\n",
      "Epoch [86/500] Batch 0/1875                       Loss D: 0.6728, loss G: 0.7830\n",
      "Epoch [87/500] Batch 0/1875                       Loss D: 0.5976, loss G: 0.9409\n",
      "Epoch [88/500] Batch 0/1875                       Loss D: 0.6630, loss G: 0.9254\n",
      "Epoch [89/500] Batch 0/1875                       Loss D: 0.4898, loss G: 1.1803\n",
      "Epoch [90/500] Batch 0/1875                       Loss D: 0.6633, loss G: 0.8636\n",
      "Epoch [91/500] Batch 0/1875                       Loss D: 0.6238, loss G: 0.9025\n",
      "Epoch [92/500] Batch 0/1875                       Loss D: 0.5329, loss G: 1.0691\n",
      "Epoch [93/500] Batch 0/1875                       Loss D: 0.5432, loss G: 1.0764\n",
      "Epoch [94/500] Batch 0/1875                       Loss D: 0.6423, loss G: 0.9318\n",
      "Epoch [95/500] Batch 0/1875                       Loss D: 0.6259, loss G: 0.9827\n",
      "Epoch [96/500] Batch 0/1875                       Loss D: 0.5534, loss G: 1.0326\n",
      "Epoch [97/500] Batch 0/1875                       Loss D: 0.5839, loss G: 0.9326\n",
      "Epoch [98/500] Batch 0/1875                       Loss D: 0.5859, loss G: 1.0121\n",
      "Epoch [99/500] Batch 0/1875                       Loss D: 0.6446, loss G: 0.9078\n",
      "Epoch [100/500] Batch 0/1875                       Loss D: 0.6721, loss G: 0.6905\n",
      "Epoch [101/500] Batch 0/1875                       Loss D: 0.5999, loss G: 1.0765\n",
      "Epoch [102/500] Batch 0/1875                       Loss D: 0.5492, loss G: 1.0612\n",
      "Epoch [103/500] Batch 0/1875                       Loss D: 0.6592, loss G: 0.7556\n",
      "Epoch [104/500] Batch 0/1875                       Loss D: 0.6314, loss G: 0.8425\n",
      "Epoch [105/500] Batch 0/1875                       Loss D: 0.5952, loss G: 1.0422\n",
      "Epoch [106/500] Batch 0/1875                       Loss D: 0.7078, loss G: 1.0982\n",
      "Epoch [107/500] Batch 0/1875                       Loss D: 0.7149, loss G: 1.0388\n",
      "Epoch [108/500] Batch 0/1875                       Loss D: 0.6099, loss G: 0.8251\n",
      "Epoch [109/500] Batch 0/1875                       Loss D: 0.6682, loss G: 0.7119\n",
      "Epoch [110/500] Batch 0/1875                       Loss D: 0.5080, loss G: 1.0753\n",
      "Epoch [111/500] Batch 0/1875                       Loss D: 0.5830, loss G: 1.0195\n",
      "Epoch [112/500] Batch 0/1875                       Loss D: 0.5668, loss G: 1.1003\n",
      "Epoch [113/500] Batch 0/1875                       Loss D: 0.6128, loss G: 1.0950\n",
      "Epoch [114/500] Batch 0/1875                       Loss D: 0.4910, loss G: 1.1279\n",
      "Epoch [115/500] Batch 0/1875                       Loss D: 0.5205, loss G: 1.0986\n",
      "Epoch [116/500] Batch 0/1875                       Loss D: 0.6342, loss G: 0.8987\n",
      "Epoch [117/500] Batch 0/1875                       Loss D: 0.5839, loss G: 0.9916\n",
      "Epoch [118/500] Batch 0/1875                       Loss D: 0.5859, loss G: 0.9556\n",
      "Epoch [119/500] Batch 0/1875                       Loss D: 0.5607, loss G: 1.0958\n",
      "Epoch [120/500] Batch 0/1875                       Loss D: 0.6350, loss G: 1.0955\n",
      "Epoch [121/500] Batch 0/1875                       Loss D: 0.6181, loss G: 0.9507\n",
      "Epoch [122/500] Batch 0/1875                       Loss D: 0.5847, loss G: 1.1062\n",
      "Epoch [123/500] Batch 0/1875                       Loss D: 0.6036, loss G: 1.0106\n",
      "Epoch [124/500] Batch 0/1875                       Loss D: 0.5985, loss G: 1.2347\n",
      "Epoch [125/500] Batch 0/1875                       Loss D: 0.5675, loss G: 1.0190\n",
      "Epoch [126/500] Batch 0/1875                       Loss D: 0.5089, loss G: 1.0761\n",
      "Epoch [127/500] Batch 0/1875                       Loss D: 0.6620, loss G: 0.9393\n",
      "Epoch [128/500] Batch 0/1875                       Loss D: 0.5889, loss G: 1.0820\n",
      "Epoch [129/500] Batch 0/1875                       Loss D: 0.6113, loss G: 1.2107\n",
      "Epoch [130/500] Batch 0/1875                       Loss D: 0.6074, loss G: 1.2137\n",
      "Epoch [131/500] Batch 0/1875                       Loss D: 0.5577, loss G: 1.1736\n",
      "Epoch [132/500] Batch 0/1875                       Loss D: 0.5913, loss G: 1.2108\n",
      "Epoch [133/500] Batch 0/1875                       Loss D: 0.5257, loss G: 1.5030\n",
      "Epoch [134/500] Batch 0/1875                       Loss D: 0.5191, loss G: 1.3949\n",
      "Epoch [135/500] Batch 0/1875                       Loss D: 0.5728, loss G: 1.1761\n",
      "Epoch [136/500] Batch 0/1875                       Loss D: 0.6672, loss G: 0.9520\n",
      "Epoch [137/500] Batch 0/1875                       Loss D: 0.6179, loss G: 0.9927\n",
      "Epoch [138/500] Batch 0/1875                       Loss D: 0.4916, loss G: 1.0602\n",
      "Epoch [139/500] Batch 0/1875                       Loss D: 0.4727, loss G: 1.0103\n",
      "Epoch [140/500] Batch 0/1875                       Loss D: 0.6426, loss G: 1.1402\n",
      "Epoch [141/500] Batch 0/1875                       Loss D: 0.5406, loss G: 1.0602\n",
      "Epoch [142/500] Batch 0/1875                       Loss D: 0.5941, loss G: 1.2411\n",
      "Epoch [143/500] Batch 0/1875                       Loss D: 0.6090, loss G: 1.0419\n",
      "Epoch [144/500] Batch 0/1875                       Loss D: 0.6132, loss G: 1.2595\n",
      "Epoch [145/500] Batch 0/1875                       Loss D: 0.5775, loss G: 1.0830\n",
      "Epoch [146/500] Batch 0/1875                       Loss D: 0.6073, loss G: 0.9966\n",
      "Epoch [147/500] Batch 0/1875                       Loss D: 0.5409, loss G: 1.3252\n",
      "Epoch [148/500] Batch 0/1875                       Loss D: 0.6133, loss G: 0.9456\n",
      "Epoch [149/500] Batch 0/1875                       Loss D: 0.5209, loss G: 1.0237\n",
      "Epoch [150/500] Batch 0/1875                       Loss D: 0.6005, loss G: 0.8474\n",
      "Epoch [151/500] Batch 0/1875                       Loss D: 0.6312, loss G: 0.9091\n",
      "Epoch [152/500] Batch 0/1875                       Loss D: 0.6072, loss G: 0.9071\n",
      "Epoch [153/500] Batch 0/1875                       Loss D: 0.6323, loss G: 1.2080\n",
      "Epoch [154/500] Batch 0/1875                       Loss D: 0.6351, loss G: 1.2091\n",
      "Epoch [155/500] Batch 0/1875                       Loss D: 0.5261, loss G: 1.1446\n",
      "Epoch [156/500] Batch 0/1875                       Loss D: 0.5559, loss G: 1.0376\n",
      "Epoch [157/500] Batch 0/1875                       Loss D: 0.5512, loss G: 1.3743\n",
      "Epoch [158/500] Batch 0/1875                       Loss D: 0.5877, loss G: 0.9614\n",
      "Epoch [159/500] Batch 0/1875                       Loss D: 0.5191, loss G: 1.2387\n",
      "Epoch [160/500] Batch 0/1875                       Loss D: 0.5447, loss G: 1.1758\n",
      "Epoch [161/500] Batch 0/1875                       Loss D: 0.5524, loss G: 1.1682\n",
      "Epoch [162/500] Batch 0/1875                       Loss D: 0.5713, loss G: 1.1847\n",
      "Epoch [163/500] Batch 0/1875                       Loss D: 0.5059, loss G: 0.9748\n",
      "Epoch [164/500] Batch 0/1875                       Loss D: 0.5716, loss G: 1.0467\n",
      "Epoch [165/500] Batch 0/1875                       Loss D: 0.5690, loss G: 1.0568\n",
      "Epoch [166/500] Batch 0/1875                       Loss D: 0.4828, loss G: 1.2733\n",
      "Epoch [167/500] Batch 0/1875                       Loss D: 0.5738, loss G: 1.2854\n",
      "Epoch [168/500] Batch 0/1875                       Loss D: 0.6431, loss G: 0.9797\n",
      "Epoch [169/500] Batch 0/1875                       Loss D: 0.5691, loss G: 1.0533\n",
      "Epoch [170/500] Batch 0/1875                       Loss D: 0.7331, loss G: 0.9140\n",
      "Epoch [171/500] Batch 0/1875                       Loss D: 0.6346, loss G: 1.3944\n",
      "Epoch [172/500] Batch 0/1875                       Loss D: 0.5971, loss G: 1.2167\n",
      "Epoch [173/500] Batch 0/1875                       Loss D: 0.6126, loss G: 1.0783\n",
      "Epoch [174/500] Batch 0/1875                       Loss D: 0.4926, loss G: 1.4964\n",
      "Epoch [175/500] Batch 0/1875                       Loss D: 0.5271, loss G: 1.1369\n",
      "Epoch [176/500] Batch 0/1875                       Loss D: 0.6464, loss G: 1.0305\n",
      "Epoch [177/500] Batch 0/1875                       Loss D: 0.5656, loss G: 1.1247\n",
      "Epoch [178/500] Batch 0/1875                       Loss D: 0.5975, loss G: 1.3395\n",
      "Epoch [179/500] Batch 0/1875                       Loss D: 0.5117, loss G: 1.0011\n",
      "Epoch [180/500] Batch 0/1875                       Loss D: 0.6769, loss G: 1.0374\n",
      "Epoch [181/500] Batch 0/1875                       Loss D: 0.6006, loss G: 0.9185\n",
      "Epoch [182/500] Batch 0/1875                       Loss D: 0.6154, loss G: 1.4487\n",
      "Epoch [183/500] Batch 0/1875                       Loss D: 0.5942, loss G: 1.2816\n",
      "Epoch [184/500] Batch 0/1875                       Loss D: 0.5140, loss G: 1.3175\n",
      "Epoch [185/500] Batch 0/1875                       Loss D: 0.4583, loss G: 1.1202\n",
      "Epoch [186/500] Batch 0/1875                       Loss D: 0.5258, loss G: 1.2824\n",
      "Epoch [187/500] Batch 0/1875                       Loss D: 0.5508, loss G: 1.2354\n",
      "Epoch [188/500] Batch 0/1875                       Loss D: 0.4866, loss G: 1.2480\n",
      "Epoch [189/500] Batch 0/1875                       Loss D: 0.5224, loss G: 1.2689\n",
      "Epoch [190/500] Batch 0/1875                       Loss D: 0.5302, loss G: 1.2314\n",
      "Epoch [191/500] Batch 0/1875                       Loss D: 0.4973, loss G: 1.2100\n",
      "Epoch [192/500] Batch 0/1875                       Loss D: 0.5425, loss G: 1.2626\n",
      "Epoch [193/500] Batch 0/1875                       Loss D: 0.6533, loss G: 1.0753\n",
      "Epoch [194/500] Batch 0/1875                       Loss D: 0.6666, loss G: 1.2587\n",
      "Epoch [195/500] Batch 0/1875                       Loss D: 0.5774, loss G: 1.2232\n",
      "Epoch [196/500] Batch 0/1875                       Loss D: 0.5670, loss G: 1.2390\n",
      "Epoch [197/500] Batch 0/1875                       Loss D: 0.4052, loss G: 1.5965\n",
      "Epoch [198/500] Batch 0/1875                       Loss D: 0.5214, loss G: 1.2122\n",
      "Epoch [199/500] Batch 0/1875                       Loss D: 0.7092, loss G: 1.0214\n",
      "Epoch [200/500] Batch 0/1875                       Loss D: 0.5203, loss G: 1.2094\n",
      "Epoch [201/500] Batch 0/1875                       Loss D: 0.4578, loss G: 1.5362\n",
      "Epoch [202/500] Batch 0/1875                       Loss D: 0.6113, loss G: 1.2192\n",
      "Epoch [203/500] Batch 0/1875                       Loss D: 0.4704, loss G: 1.4973\n",
      "Epoch [204/500] Batch 0/1875                       Loss D: 0.5358, loss G: 1.0044\n",
      "Epoch [205/500] Batch 0/1875                       Loss D: 0.5693, loss G: 0.8729\n",
      "Epoch [206/500] Batch 0/1875                       Loss D: 0.4950, loss G: 0.9711\n",
      "Epoch [207/500] Batch 0/1875                       Loss D: 0.5045, loss G: 1.1235\n",
      "Epoch [208/500] Batch 0/1875                       Loss D: 0.5402, loss G: 1.2624\n",
      "Epoch [209/500] Batch 0/1875                       Loss D: 0.5558, loss G: 1.3675\n",
      "Epoch [210/500] Batch 0/1875                       Loss D: 0.4528, loss G: 1.5034\n",
      "Epoch [211/500] Batch 0/1875                       Loss D: 0.4550, loss G: 1.1256\n",
      "Epoch [212/500] Batch 0/1875                       Loss D: 0.6117, loss G: 0.9524\n",
      "Epoch [213/500] Batch 0/1875                       Loss D: 0.5477, loss G: 0.9919\n",
      "Epoch [214/500] Batch 0/1875                       Loss D: 0.5816, loss G: 1.0609\n",
      "Epoch [215/500] Batch 0/1875                       Loss D: 0.5127, loss G: 1.0125\n",
      "Epoch [216/500] Batch 0/1875                       Loss D: 0.6057, loss G: 0.8940\n",
      "Epoch [217/500] Batch 0/1875                       Loss D: 0.6698, loss G: 1.1306\n",
      "Epoch [218/500] Batch 0/1875                       Loss D: 0.5311, loss G: 1.2686\n",
      "Epoch [219/500] Batch 0/1875                       Loss D: 0.6007, loss G: 1.1121\n",
      "Epoch [220/500] Batch 0/1875                       Loss D: 0.5074, loss G: 1.2139\n",
      "Epoch [221/500] Batch 0/1875                       Loss D: 0.5120, loss G: 1.2260\n",
      "Epoch [222/500] Batch 0/1875                       Loss D: 0.5240, loss G: 1.1466\n",
      "Epoch [223/500] Batch 0/1875                       Loss D: 0.6130, loss G: 0.8951\n",
      "Epoch [224/500] Batch 0/1875                       Loss D: 0.7239, loss G: 0.8012\n",
      "Epoch [225/500] Batch 0/1875                       Loss D: 0.4674, loss G: 1.3027\n",
      "Epoch [226/500] Batch 0/1875                       Loss D: 0.5434, loss G: 1.2162\n",
      "Epoch [227/500] Batch 0/1875                       Loss D: 0.5982, loss G: 1.3106\n",
      "Epoch [228/500] Batch 0/1875                       Loss D: 0.5439, loss G: 1.0102\n",
      "Epoch [229/500] Batch 0/1875                       Loss D: 0.5950, loss G: 1.5041\n",
      "Epoch [230/500] Batch 0/1875                       Loss D: 0.5405, loss G: 1.1621\n",
      "Epoch [231/500] Batch 0/1875                       Loss D: 0.5089, loss G: 1.4306\n",
      "Epoch [232/500] Batch 0/1875                       Loss D: 0.6124, loss G: 1.0678\n",
      "Epoch [233/500] Batch 0/1875                       Loss D: 0.5591, loss G: 1.3469\n",
      "Epoch [234/500] Batch 0/1875                       Loss D: 0.5134, loss G: 1.2630\n",
      "Epoch [235/500] Batch 0/1875                       Loss D: 0.5256, loss G: 1.5981\n",
      "Epoch [236/500] Batch 0/1875                       Loss D: 0.5559, loss G: 1.2809\n",
      "Epoch [237/500] Batch 0/1875                       Loss D: 0.5291, loss G: 1.2723\n",
      "Epoch [238/500] Batch 0/1875                       Loss D: 0.4951, loss G: 1.2876\n",
      "Epoch [239/500] Batch 0/1875                       Loss D: 0.5323, loss G: 1.4357\n",
      "Epoch [240/500] Batch 0/1875                       Loss D: 0.6009, loss G: 1.2072\n",
      "Epoch [241/500] Batch 0/1875                       Loss D: 0.5813, loss G: 1.3326\n",
      "Epoch [242/500] Batch 0/1875                       Loss D: 0.5930, loss G: 1.0918\n",
      "Epoch [243/500] Batch 0/1875                       Loss D: 0.4735, loss G: 1.5153\n",
      "Epoch [244/500] Batch 0/1875                       Loss D: 0.5626, loss G: 1.1622\n",
      "Epoch [245/500] Batch 0/1875                       Loss D: 0.5637, loss G: 1.2978\n",
      "Epoch [246/500] Batch 0/1875                       Loss D: 0.6940, loss G: 0.8361\n",
      "Epoch [247/500] Batch 0/1875                       Loss D: 0.4903, loss G: 1.2594\n",
      "Epoch [248/500] Batch 0/1875                       Loss D: 0.5802, loss G: 1.1793\n",
      "Epoch [249/500] Batch 0/1875                       Loss D: 0.5515, loss G: 1.1427\n",
      "Epoch [250/500] Batch 0/1875                       Loss D: 0.5844, loss G: 1.2742\n",
      "Epoch [251/500] Batch 0/1875                       Loss D: 0.5269, loss G: 1.4237\n",
      "Epoch [252/500] Batch 0/1875                       Loss D: 0.4258, loss G: 1.3589\n",
      "Epoch [253/500] Batch 0/1875                       Loss D: 0.5282, loss G: 1.1574\n",
      "Epoch [254/500] Batch 0/1875                       Loss D: 0.5057, loss G: 1.2285\n",
      "Epoch [255/500] Batch 0/1875                       Loss D: 0.6072, loss G: 1.0939\n",
      "Epoch [256/500] Batch 0/1875                       Loss D: 0.5896, loss G: 1.3127\n",
      "Epoch [257/500] Batch 0/1875                       Loss D: 0.5338, loss G: 1.1917\n",
      "Epoch [258/500] Batch 0/1875                       Loss D: 0.4244, loss G: 1.2539\n",
      "Epoch [259/500] Batch 0/1875                       Loss D: 0.4467, loss G: 1.1211\n",
      "Epoch [260/500] Batch 0/1875                       Loss D: 0.6029, loss G: 1.0820\n",
      "Epoch [261/500] Batch 0/1875                       Loss D: 0.5795, loss G: 1.0281\n",
      "Epoch [262/500] Batch 0/1875                       Loss D: 0.4522, loss G: 1.3830\n",
      "Epoch [263/500] Batch 0/1875                       Loss D: 0.5582, loss G: 1.2540\n",
      "Epoch [264/500] Batch 0/1875                       Loss D: 0.5797, loss G: 1.1930\n",
      "Epoch [265/500] Batch 0/1875                       Loss D: 0.5540, loss G: 1.4775\n",
      "Epoch [266/500] Batch 0/1875                       Loss D: 0.5855, loss G: 1.0048\n",
      "Epoch [267/500] Batch 0/1875                       Loss D: 0.4877, loss G: 1.1445\n",
      "Epoch [268/500] Batch 0/1875                       Loss D: 0.5141, loss G: 1.0879\n",
      "Epoch [269/500] Batch 0/1875                       Loss D: 0.5814, loss G: 1.3663\n",
      "Epoch [270/500] Batch 0/1875                       Loss D: 0.6569, loss G: 1.0873\n",
      "Epoch [271/500] Batch 0/1875                       Loss D: 0.4802, loss G: 1.2972\n",
      "Epoch [272/500] Batch 0/1875                       Loss D: 0.5878, loss G: 1.5713\n",
      "Epoch [273/500] Batch 0/1875                       Loss D: 0.3975, loss G: 1.4829\n",
      "Epoch [274/500] Batch 0/1875                       Loss D: 0.5120, loss G: 1.1722\n",
      "Epoch [275/500] Batch 0/1875                       Loss D: 0.4483, loss G: 1.4804\n",
      "Epoch [276/500] Batch 0/1875                       Loss D: 0.4703, loss G: 1.4943\n",
      "Epoch [277/500] Batch 0/1875                       Loss D: 0.6058, loss G: 1.0353\n",
      "Epoch [278/500] Batch 0/1875                       Loss D: 0.5971, loss G: 1.0874\n",
      "Epoch [279/500] Batch 0/1875                       Loss D: 0.5419, loss G: 1.1180\n",
      "Epoch [280/500] Batch 0/1875                       Loss D: 0.5541, loss G: 1.0468\n",
      "Epoch [281/500] Batch 0/1875                       Loss D: 0.5352, loss G: 1.0499\n",
      "Epoch [282/500] Batch 0/1875                       Loss D: 0.6554, loss G: 0.8749\n",
      "Epoch [283/500] Batch 0/1875                       Loss D: 0.5446, loss G: 1.2566\n",
      "Epoch [284/500] Batch 0/1875                       Loss D: 0.6467, loss G: 0.8451\n",
      "Epoch [285/500] Batch 0/1875                       Loss D: 0.6643, loss G: 1.1203\n",
      "Epoch [286/500] Batch 0/1875                       Loss D: 0.5749, loss G: 1.3539\n",
      "Epoch [287/500] Batch 0/1875                       Loss D: 0.4868, loss G: 1.2958\n",
      "Epoch [288/500] Batch 0/1875                       Loss D: 0.4834, loss G: 1.1432\n",
      "Epoch [289/500] Batch 0/1875                       Loss D: 0.5299, loss G: 1.1005\n",
      "Epoch [290/500] Batch 0/1875                       Loss D: 0.5877, loss G: 1.0438\n",
      "Epoch [291/500] Batch 0/1875                       Loss D: 0.5929, loss G: 1.1976\n",
      "Epoch [292/500] Batch 0/1875                       Loss D: 0.5501, loss G: 1.0700\n",
      "Epoch [293/500] Batch 0/1875                       Loss D: 0.6031, loss G: 1.2646\n",
      "Epoch [294/500] Batch 0/1875                       Loss D: 0.4251, loss G: 1.3518\n",
      "Epoch [295/500] Batch 0/1875                       Loss D: 0.4519, loss G: 1.2102\n",
      "Epoch [296/500] Batch 0/1875                       Loss D: 0.5477, loss G: 1.3799\n",
      "Epoch [297/500] Batch 0/1875                       Loss D: 0.5495, loss G: 1.3072\n",
      "Epoch [298/500] Batch 0/1875                       Loss D: 0.5050, loss G: 1.4968\n",
      "Epoch [299/500] Batch 0/1875                       Loss D: 0.5362, loss G: 1.1678\n",
      "Epoch [300/500] Batch 0/1875                       Loss D: 0.4661, loss G: 1.4051\n",
      "Epoch [301/500] Batch 0/1875                       Loss D: 0.5631, loss G: 1.1839\n",
      "Epoch [302/500] Batch 0/1875                       Loss D: 0.5484, loss G: 1.3224\n",
      "Epoch [303/500] Batch 0/1875                       Loss D: 0.4962, loss G: 1.3504\n",
      "Epoch [304/500] Batch 0/1875                       Loss D: 0.5263, loss G: 1.3351\n",
      "Epoch [305/500] Batch 0/1875                       Loss D: 0.5472, loss G: 1.6619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DevStudio\\Python learning & projects\\practicevenv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real,_) in enumerate(loader):\n",
    "        real = real.view(-1,784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        # training the discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size,z_dimension).to(device)\n",
    "        fake = generator(noise)\n",
    "        disc_real = discriminator(real).view(-1)\n",
    "        # ℓ(x,y)=L={l1,…,lN}⊤,  ln​ = −wn​ [yn​ ⋅log xn​ +(1−yn​ )⋅log(1−xn​ )], second term canceled out\n",
    "        lossD_real = criterion(disc_real,torch.ones_like(disc_real))\n",
    "        disc_fake = discriminator(fake).view(-1)\n",
    "        # ℓ(x,y)=L={l1,…,lN}⊤,  ln​ = −wn​ [yn​ ⋅log xn​ +(1−yn​ )⋅log(1−xn​ )], first term canceled out\n",
    "        lossD_fake = criterion(disc_fake,torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_fake + lossD_real)/2\n",
    "        discriminator.zero_grad()\n",
    "        lossD.backward(retain_graph = True)\n",
    "        optim_disc.step()\n",
    "\n",
    "        # training the generator: min log(1 - D(G(z))) -> max log(D(G(z)))\n",
    "        output = discriminator(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "        optim_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes that may improve results\n",
    "\n",
    "As we know that simple GANs are sensitive to hyperparameters, there are some changes that may significantly affect model performance:\n",
    "\n",
    "1) Using larger network\n",
    "2) Better normalization with BatchNorm\n",
    "3) Different learning rate\n",
    "4) Change of architecture to DCGAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
